<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Playground: Learning by Building</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        h1 {
            color: #0056b3;
        }
        .container {
            background-color: #fff;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            max-width: 900px;
            margin: 20px auto;
        }
        .nav-menu {
            list-style: none;
            padding: 0;
            display: flex;
            gap: 20px;
            margin-bottom: 30px;
            border-bottom: 1px solid #eee;
            padding-bottom: 10px;
        }
        .nav-menu li a {
            text-decoration: none;
            color: #007bff;
            font-weight: bold;
        }
        .nav-menu li a:hover {
            color: #0056b3;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Welcome to the LLM Playground!</h1>
        <p>This is where we'll learn about Large Language Models by building a hands-on interactive environment.</p>

        <ul class="nav-menu">
            <li><a href="#">Overview</a></li>
            <li><a href="#">Data</a></li>
            <li><a href="#">Tokenization</a></li>
            <li><a href="#">Architecture</a></li>
            <li><a href="#">Generation</a></li>
            <li><a href="#">Evaluation</a></li>
        </ul>

        <section id="overview">
            <h2>Project Overview</h2>
            <p>
                Our journey begins! We'll explore the fascinating world of Large Language Models (LLMs) from their foundational concepts to advanced training techniques.
                Each step will involve building a component of this playground, writing an article, and committing our progress to GitHub.
            </p>
            <p>
                **Current Module:** Module 1 - Project Kick-off & Environment Setup
            </p>
        </section>

        <section id="llm-overview">
            <h2>Module 2: LLM Overview and Foundations</h2>
            <p>
                Understanding Large Language Models (LLMs) starts with grasping their fundamental task: **language modeling**. This involves predicting the next word or token in a sequence, a seemingly simple task that, when scaled, enables incredible capabilities.
            </p>
            <p>
                The "largeness" of LLMs refers to their massive number of parameters (billions, even trillions) and the colossal datasets they are trained on. This foundation allows them to learn complex patterns, grammar, semantics, and world knowledge.
            </p>
            <h3>Conceptual Overview:</h3>
            <div style="text-align: center; margin: 20px 0;">
                <img src="img/llm-overview-bytebytego.png" alt="LLM Overview and Language Modeling Diagram" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                <p style="font-size: 0.9em; color: #666; margin-top: 10px;">
                    *A high-level view of how massive text data feeds into an LLM, enabling it to perform language modeling â€“ predicting the next token in a sequence.*
                </p>
            </div>
            <p>
                This diagram, illustrates how raw text data fuels the LLM, whose core function is to generate coherent text by mastering next-token prediction.
            </p>
        </section>

        <!-- ... rest of your index.html content ... -->

            <!-- ... rest of your index.html content ... -->

        <!-- ... rest of your index.html content ... -->
    </div>
</body>
</html>