<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Playground: Learning by Building</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        h1 {
            color: #0056b3;
        }
        .container {
            background-color: #fff;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            max-width: 900px;
            margin: 20px auto;
        }
        .nav-menu {
            list-style: none;
            padding: 0;
            display: flex;
            gap: 20px;
            margin-bottom: 30px;
            border-bottom: 1px solid #eee;
            padding-bottom: 10px;
        }
        .nav-menu li a {
            text-decoration: none;
            color: #007bff;
            font-weight: bold;
        }
        .nav-menu li a:hover {
            color: #0056b3;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Welcome to the LLM Playground!</h1>
        <p>This is where we'll learn about Large Language Models by building a hands-on interactive environment.</p>

        <ul class="nav-menu">
            <li><a href="#">Overview</a></li>
            <li><a href="#">Data</a></li>
            <li><a href="#">Tokenization</a></li>
            <li><a href="#">Architecture</a></li>
            <li><a href="#">Generation</a></li>
            <li><a href="#">Evaluation</a></li>
        </ul>

        <section id="overview">
            <h2>Project Overview</h2>
            <p>
                Our journey begins! We'll explore the fascinating world of Large Language Models (LLMs) from their foundational concepts to advanced training techniques.
                Each step will involve building a component of this playground, writing an article, and committing our progress to GitHub.
            </p>
            <p>
                **Current Module:** Module 1 - Project Kick-off & Environment Setup
            </p>
        </section>

        <section id="llm-overview">
            <h2>Module 2: LLM Overview and Foundations</h2>
            <p>
                Understanding Large Language Models (LLMs) starts with grasping their fundamental task: **language modeling**. This involves predicting the next word or token in a sequence, a seemingly simple task that, when scaled, enables incredible capabilities.
            </p>
            <p>
                The "largeness" of LLMs refers to their massive number of parameters (billions, even trillions) and the colossal datasets they are trained on. This foundation allows them to learn complex patterns, grammar, semantics, and world knowledge.
            </p>
            <h3>Conceptual Overview:</h3>
            <div style="text-align: center; margin: 20px 0;">
                <img src="img/llm-overview-bytebytego.png" alt="LLM Overview and Language Modeling Diagram" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                <p style="font-size: 0.9em; color: #666; margin-top: 10px;">
                    *A high-level view of how massive text data feeds into an LLM, enabling it to perform language modeling â€“ predicting the next token in a sequence.*
                </p>
            </div>
            <p>
                This diagram, illustrates how raw text data fuels the LLM, whose core function is to generate coherent text by mastering next-token prediction.
            </p>
        </section>

        <section id="data-collection">
            <h2>Module 3: Data Collection - The Fuel for LLMs</h2>
            <p>
                The incredible capabilities of Large Language Models stem directly from the vast amounts of text data they are trained on during their **pre-training** phase. This data acts as their "knowledge base" and teaches them the patterns and nuances of human language.
            </p>
            <p>
                Data collection involves gathering petabytes of raw text from diverse sources, each presenting its own challenges related to scale, noise, and potential biases.
            </p>
            <h3>Data Collection Pipeline:</h3>
            <div style="text-align: center; margin: 20px 0;">
                <img src="img/llm-data-collection-bytebytego.png" alt="LLM Data Collection Diagram" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                <p style="font-size: 0.9em; color: #666; margin-top: 10px;">
                    *An overview of the data pipeline for LLMs, from massive raw sources to the final training dataset, highlighting common challenges.*
                </p>
            </div>
            <p>
                Understanding these sources and the inherent challenges is crucial for appreciating the complexity behind building powerful and robust LLMs. The quality and diversity of this initial data profoundly impact the model's eventual performance and behavior.
            </p>
        </section>


        <section id="data-cleaning">
            <h2>Module 4: Data Cleaning - Refining the Raw</h2>
            <p>
                The massive datasets collected from the internet are inherently "noisy," containing duplicates, low-quality content, boilerplate text, and potential biases. **Data cleaning** is a crucial pre-training step that transforms this raw data into a high-quality, usable dataset, directly impacting an LLM's performance, efficiency, and alignment.
            </p>
            <p>
                This process involves various techniques, from deduplication and boilerplate removal to sophisticated filtering of low-quality or non-target language content.
            </p>
            <h3>Data Cleaning Pipeline (ByteByteGo Style):</h3>
            <div style="text-align: center; margin: 20px 0;">
                <img src="img/llm-data-cleaning-bytebytego.png" alt="LLM Data Cleaning Diagram" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                <p style="font-size: 0.9em; color: #666; margin-top: 10px;">
                    *An illustration of the essential data cleaning steps, transforming raw internet text into a refined dataset suitable for LLM training.*
                </p>
            </div>
            <p>
                Pioneering efforts like RefinedWeb, Dolma, and FineWeb demonstrate the value of meticulously cleaned datasets in building state-of-the-art LLMs, emphasizing that data quality is as vital as quantity.
            </p>
        </section>


    </div>
</body>
</html>